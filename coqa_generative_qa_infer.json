{
  "chainer": {
    "in": ["question", "contexts"],
    "pipe": [
      {
        "class_name": "torch_transformers_generative_qa_preprocessor",
        "vocab_file": "{TRANSFORMER}",
        "max_seq_length": 512,
        "in": ["question", "contexts"],
        "out": ["input_ids", "attention_mask"]
      },
      {
        "class_name": "torch_generative_qa",
        "pretrained_transformer": "{TRANSFORMER}",
        "save_path": "{MODEL_PATH}/model",
        "load_path": "{MODEL_PATH}/model",
        "optimizer": "AdamW",
        "optimizer_parameters": {
          "lr": 1e-04,
          "weight_decay": 0.01,
          "betas": [0.9, 0.999],
          "eps": 1e-06
        },
        "learning_rate_drop_patience": 2,
        "learning_rate_drop_div": 2.0,
        "in": ["input_ids", "attention_mask"],
        "out": ["answer"]
      }
    ],
    "out": ["answer"]
  },
  "train": {
    "show_examples": false,
    "evaluation_targets": [
      "valid"
    ],
    "log_every_n_batches": 50,
    "val_every_n_batches": 500,
    "batch_size": 15,
    "pytest_max_batches": 2,
    "pytest_batch_size": 5,
    "validation_patience": 10,
    "metrics": [
      {
        "name": "ppl",
        "inputs": ["ppl"]
      },
      {
        "name": "sacred_bleu",
        "inputs": ["target", "answer"]
      }
    ],
    "class_name": "torch_trainer"
  },
  "metadata": {
    "variables": {
      "MAX_TOKENS" : 50,
      "LOWERCASE": false,
      "TRANSFORMER": "t5-base",
      "ROOT_PATH": "~/.deeppavlov",
      "DOWNLOADS_PATH": "{ROOT_PATH}/downloads",
      "MODELS_PATH": "{ROOT_PATH}/models",
      "MODEL_PATH": "{MODELS_PATH}/generative_qa/first_train/coqa-{TRANSFORMER}-max-tok-{MAX_TOKENS}"
    }
  }
}
